{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('datasets.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')"
      ],
      "metadata": {
        "id": "HHw1QFCLUD9s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x7UhAWxy3GW-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from os import path as osp\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class VITONDataset(data.Dataset):\n",
        "    def __init__(self, opt):\n",
        "        super(VITONDataset, self).__init__()\n",
        "        self.load_height = opt.load_height\n",
        "        self.load_width = opt.load_width\n",
        "        self.semantic_nc = opt.semantic_nc\n",
        "        self.data_path = osp.join(opt.dataset_dir, opt.dataset_mode)\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "        # load data list\n",
        "        img_names = []\n",
        "        c_names = []\n",
        "        with open(osp.join(opt.dataset_dir, opt.dataset_list), 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                img_name, c_name = line.strip().split()\n",
        "                img_names.append(img_name)\n",
        "                c_names.append(c_name)\n",
        "\n",
        "        self.img_names = img_names\n",
        "        self.c_names = dict()\n",
        "        self.c_names['unpaired'] = c_names\n",
        "\n",
        "    def get_parse_agnostic(self, parse, pose_data):\n",
        "        parse_array = np.array(parse)\n",
        "        parse_upper = ((parse_array == 5).astype(np.float32) +\n",
        "                       (parse_array == 6).astype(np.float32) +\n",
        "                       (parse_array == 7).astype(np.float32))\n",
        "        parse_neck = (parse_array == 10).astype(np.float32)\n",
        "\n",
        "        r = 10\n",
        "        agnostic = parse.copy()\n",
        "\n",
        "        # mask arms\n",
        "        for parse_id, pose_ids in [(14, [2, 5, 6, 7]), (15, [5, 2, 3, 4])]:\n",
        "            mask_arm = Image.new('L', (self.load_width, self.load_height), 'black')\n",
        "            mask_arm_draw = ImageDraw.Draw(mask_arm)\n",
        "            i_prev = pose_ids[0]\n",
        "            for i in pose_ids[1:]:\n",
        "                if (pose_data[i_prev, 0] == 0.0 and pose_data[i_prev, 1] == 0.0) or (pose_data[i, 0] == 0.0 and pose_data[i, 1] == 0.0):\n",
        "                    continue\n",
        "                mask_arm_draw.line([tuple(pose_data[j]) for j in [i_prev, i]], 'white', width=r*10)\n",
        "                pointx, pointy = pose_data[i]\n",
        "                radius = r*4 if i == pose_ids[-1] else r*15\n",
        "                mask_arm_draw.ellipse((pointx-radius, pointy-radius, pointx+radius, pointy+radius), 'white', 'white')\n",
        "                i_prev = i\n",
        "            parse_arm = (np.array(mask_arm) / 255) * (parse_array == parse_id).astype(np.float32)\n",
        "            agnostic.paste(0, None, Image.fromarray(np.uint8(parse_arm * 255), 'L'))\n",
        "\n",
        "        # mask torso & neck\n",
        "        agnostic.paste(0, None, Image.fromarray(np.uint8(parse_upper * 255), 'L'))\n",
        "        agnostic.paste(0, None, Image.fromarray(np.uint8(parse_neck * 255), 'L'))\n",
        "\n",
        "        return agnostic\n",
        "\n",
        "    def get_img_agnostic(self, img, parse, pose_data):\n",
        "        parse_array = np.array(parse)\n",
        "        parse_head = ((parse_array == 4).astype(np.float32) +\n",
        "                      (parse_array == 13).astype(np.float32))\n",
        "        parse_lower = ((parse_array == 9).astype(np.float32) +\n",
        "                       (parse_array == 12).astype(np.float32) +\n",
        "                       (parse_array == 16).astype(np.float32) +\n",
        "                       (parse_array == 17).astype(np.float32) +\n",
        "                       (parse_array == 18).astype(np.float32) +\n",
        "                       (parse_array == 19).astype(np.float32))\n",
        "\n",
        "        r = 20\n",
        "        agnostic = img.copy()\n",
        "        agnostic_draw = ImageDraw.Draw(agnostic)\n",
        "\n",
        "        length_a = np.linalg.norm(pose_data[5] - pose_data[2])\n",
        "        length_b = np.linalg.norm(pose_data[12] - pose_data[9])\n",
        "        point = (pose_data[9] + pose_data[12]) / 2\n",
        "        pose_data[9] = point + (pose_data[9] - point) / length_b * length_a\n",
        "        pose_data[12] = point + (pose_data[12] - point) / length_b * length_a\n",
        "\n",
        "        # mask arms\n",
        "        agnostic_draw.line([tuple(pose_data[i]) for i in [2, 5]], 'gray', width=r*10)\n",
        "        for i in [2, 5]:\n",
        "            pointx, pointy = pose_data[i]\n",
        "            agnostic_draw.ellipse((pointx-r*5, pointy-r*5, pointx+r*5, pointy+r*5), 'gray', 'gray')\n",
        "        for i in [3, 4, 6, 7]:\n",
        "            if (pose_data[i - 1, 0] == 0.0 and pose_data[i - 1, 1] == 0.0) or (pose_data[i, 0] == 0.0 and pose_data[i, 1] == 0.0):\n",
        "                continue\n",
        "            agnostic_draw.line([tuple(pose_data[j]) for j in [i - 1, i]], 'gray', width=r*10)\n",
        "            pointx, pointy = pose_data[i]\n",
        "            agnostic_draw.ellipse((pointx-r*5, pointy-r*5, pointx+r*5, pointy+r*5), 'gray', 'gray')\n",
        "\n",
        "        # mask torso\n",
        "        for i in [9, 12]:\n",
        "            pointx, pointy = pose_data[i]\n",
        "            agnostic_draw.ellipse((pointx-r*3, pointy-r*6, pointx+r*3, pointy+r*6), 'gray', 'gray')\n",
        "        agnostic_draw.line([tuple(pose_data[i]) for i in [2, 9]], 'gray', width=r*6)\n",
        "        agnostic_draw.line([tuple(pose_data[i]) for i in [5, 12]], 'gray', width=r*6)\n",
        "        agnostic_draw.line([tuple(pose_data[i]) for i in [9, 12]], 'gray', width=r*12)\n",
        "        agnostic_draw.polygon([tuple(pose_data[i]) for i in [2, 5, 12, 9]], 'gray', 'gray')\n",
        "\n",
        "        # mask neck\n",
        "        pointx, pointy = pose_data[1]\n",
        "        agnostic_draw.rectangle((pointx-r*7, pointy-r*7, pointx+r*7, pointy+r*7), 'gray', 'gray')\n",
        "        agnostic.paste(img, None, Image.fromarray(np.uint8(parse_head * 255), 'L'))\n",
        "        agnostic.paste(img, None, Image.fromarray(np.uint8(parse_lower * 255), 'L'))\n",
        "\n",
        "        return agnostic\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.img_names[index]\n",
        "        c_name = {}\n",
        "        c = {}\n",
        "        cm = {}\n",
        "        for key in self.c_names:\n",
        "            c_name[key] = self.c_names[key][index]\n",
        "            c[key] = Image.open(osp.join(self.data_path, 'cloth', c_name[key])).convert('RGB')\n",
        "            c[key] = transforms.Resize(self.load_width, interpolation=2)(c[key])\n",
        "            cm[key] = Image.open(osp.join(self.data_path, 'cloth-mask', c_name[key]))\n",
        "            cm[key] = transforms.Resize(self.load_width, interpolation=0)(cm[key])\n",
        "\n",
        "            c[key] = self.transform(c[key])  # [-1,1]\n",
        "            cm_array = np.array(cm[key])\n",
        "            cm_array = (cm_array >= 128).astype(np.float32)\n",
        "            cm[key] = torch.from_numpy(cm_array)  # [0,1]\n",
        "            cm[key].unsqueeze_(0)\n",
        "\n",
        "        # load pose image\n",
        "        pose_name = img_name.replace('.jpg', '_rendered.png')\n",
        "        pose_rgb = Image.open(osp.join(self.data_path, 'openpose-img', pose_name))\n",
        "        pose_rgb = transforms.Resize(self.load_width, interpolation=2)(pose_rgb)\n",
        "        pose_rgb = self.transform(pose_rgb)  # [-1,1]\n",
        "\n",
        "        pose_name = img_name.replace('.jpg', '_keypoints.json')\n",
        "        with open(osp.join(self.data_path, 'openpose-json', pose_name), 'r') as f:\n",
        "            pose_label = json.load(f)\n",
        "            pose_data = pose_label['people'][0]['pose_keypoints_2d']\n",
        "            pose_data = np.array(pose_data)\n",
        "            pose_data = pose_data.reshape((-1, 3))[:, :2]\n",
        "\n",
        "        # load parsing image\n",
        "        parse_name = img_name.replace('.jpg', '.png')\n",
        "        parse = Image.open(osp.join(self.data_path, 'image-parse', parse_name))\n",
        "        parse = transforms.Resize(self.load_width, interpolation=0)(parse)\n",
        "        parse_agnostic = self.get_parse_agnostic(parse, pose_data)\n",
        "        parse_agnostic = torch.from_numpy(np.array(parse_agnostic)[None]).long()\n",
        "\n",
        "        labels = {\n",
        "            0: ['background', [0, 10]],\n",
        "            1: ['hair', [1, 2]],\n",
        "            2: ['face', [4, 13]],\n",
        "            3: ['upper', [5, 6, 7]],\n",
        "            4: ['bottom', [9, 12]],\n",
        "            5: ['left_arm', [14]],\n",
        "            6: ['right_arm', [15]],\n",
        "            7: ['left_leg', [16]],\n",
        "            8: ['right_leg', [17]],\n",
        "            9: ['left_shoe', [18]],\n",
        "            10: ['right_shoe', [19]],\n",
        "            11: ['socks', [8]],\n",
        "            12: ['noise', [3, 11]]\n",
        "        }\n",
        "        parse_agnostic_map = torch.zeros(20, self.load_height, self.load_width, dtype=torch.float)\n",
        "        parse_agnostic_map.scatter_(0, parse_agnostic, 1.0)\n",
        "        new_parse_agnostic_map = torch.zeros(self.semantic_nc, self.load_height, self.load_width, dtype=torch.float)\n",
        "        for i in range(len(labels)):\n",
        "            for label in labels[i][1]:\n",
        "                new_parse_agnostic_map[i] += parse_agnostic_map[label]\n",
        "\n",
        "        # load person image\n",
        "        img = Image.open(osp.join(self.data_path, 'image', img_name))\n",
        "        img = transforms.Resize(self.load_width, interpolation=2)(img)\n",
        "        img_agnostic = self.get_img_agnostic(img, parse, pose_data)\n",
        "        img = self.transform(img)\n",
        "        img_agnostic = self.transform(img_agnostic)  # [-1,1]\n",
        "\n",
        "        result = {\n",
        "            'img_name': img_name,\n",
        "            'c_name': c_name,\n",
        "            'img': img,\n",
        "            'img_agnostic': img_agnostic,\n",
        "            'parse_agnostic': new_parse_agnostic_map,\n",
        "            'pose': pose_rgb,\n",
        "            'cloth': c,\n",
        "            'cloth_mask': cm,\n",
        "        }\n",
        "        return result\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "\n",
        "class VITONDataLoader:\n",
        "    def __init__(self, opt, dataset):\n",
        "        super(VITONDataLoader, self).__init__()\n",
        "\n",
        "        self.data_loader = data.DataLoader(\n",
        "                dataset, batch_size=opt.batch_size, shuffle=None,\n",
        "                num_workers=opt.workers, pin_memory=True, drop_last=True, sampler=None\n",
        "        )\n",
        "        self.dataset = dataset\n",
        "        self.data_iter = self.data_loader.__iter__()\n",
        "\n",
        "    def next_batch(self):\n",
        "        try:\n",
        "            batch = self.data_iter.__next__()\n",
        "        except StopIteration:\n",
        "            self.data_iter = self.data_loader.__iter__()\n",
        "            batch = self.data_iter.__next__()\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Options:\n",
        "    def __init__(self):\n",
        "        self.load_height = 1024\n",
        "        self.load_width = 768\n",
        "        self.semantic_nc = 13\n",
        "        self.dataset_dir = r'datasets/'\n",
        "        self.dataset_mode = r'test'\n",
        "        self.dataset_list = r'test_pairs.txt'\n",
        "        self.batch_size = 1\n",
        "        self.workers = 1\n",
        "        self.semantic_nc = 13\n",
        "        self.init_type = 'xavier'\n",
        "        self.init_variance = 0.\n",
        "        self.checkpoint_dir = r'./checkpoints/'\n",
        "        self.seg_checkpoint = r'seg_final.pth'\n",
        "opt = Options()"
      ],
      "metadata": {
        "id": "hRHBmwE134vG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w05PxW1KpGxQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_dataset = VITONDataset(opt)\n",
        "test_loader = VITONDataLoader(opt, test_dataset)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, inputs in enumerate(test_loader.data_loader):\n",
        "        img_names = inputs['img_name']\n",
        "        c_names = inputs['c_name']['unpaired']\n",
        "\n",
        "        img_agnostic = inputs['img_agnostic']\n",
        "        parse_agnostic = inputs['parse_agnostic']\n",
        "        pose = inputs['pose']\n",
        "        c = inputs['cloth']['unpaired']\n",
        "        cm = inputs['cloth_mask']['unpaired']\n"
      ],
      "metadata": {
        "id": "m-jN3hRr3Zk6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_noise(shape):\n",
        "    noise = np.zeros(shape, dtype=np.uint8)\n",
        "    ### noise\n",
        "    noise = cv2.randn(noise, 0, 255)\n",
        "    noise = np.asarray(noise / 255, dtype=np.uint8)\n",
        "    noise = torch.tensor(noise, dtype=torch.float32)\n",
        "    return noise"
      ],
      "metadata": {
        "id": "cPqsJO6CXc9F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "import cv2\n",
        "parse_agnostic_down = F.interpolate(parse_agnostic, size=(256, 192), mode='bilinear')\n",
        "pose_down = F.interpolate(pose, size=(256, 192), mode='bilinear')\n",
        "c_masked_down = F.interpolate(c * cm, size=(256, 192), mode='bilinear')\n",
        "cm_down = F.interpolate(cm, size=(256, 192), mode='bilinear')\n",
        "seg_input = torch.cat((cm_down, c_masked_down, parse_agnostic_down, pose_down, gen_noise(cm_down.size())), dim=1)"
      ],
      "metadata": {
        "id": "Z8pTcj6pXLBF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import init\n",
        "! pip install torchgeometry\n",
        "import torchgeometry as tgm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8tAQNCEdnXx",
        "outputId": "0a86c8de-1080-4c6a-e135-6f916a75dec2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchgeometry\n",
            "  Downloading torchgeometry-0.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from torchgeometry) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->torchgeometry) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->torchgeometry) (1.3.0)\n",
            "Installing collected packages: torchgeometry\n",
            "Successfully installed torchgeometry-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseNetwork, self).__init__()\n",
        "\n",
        "    def print_network(self):\n",
        "        num_params = 0\n",
        "        for param in self.parameters():\n",
        "            num_params += param.numel()\n",
        "        print(\"Network [{}] was created. Total number of parameters: {:.1f} million. \"\n",
        "              \"To see the architecture, do print(network).\".format(self.__class__.__name__, num_params / 1000000))\n",
        "\n",
        "    def init_weights(self, init_type='normal', gain=0.02):\n",
        "        def init_func(m):\n",
        "            classname = m.__class__.__name__\n",
        "            if 'BatchNorm2d' in classname:\n",
        "                if hasattr(m, 'weight') and m.weight is not None:\n",
        "                    init.normal_(m.weight.data, 1.0, gain)\n",
        "                if hasattr(m, 'bias') and m.bias is not None:\n",
        "                    init.constant_(m.bias.data, 0.0)\n",
        "            elif ('Conv' in classname or 'Linear' in classname) and hasattr(m, 'weight'):\n",
        "                if init_type == 'normal':\n",
        "                    init.normal_(m.weight.data, 0.0, gain)\n",
        "                elif init_type == 'xavier':\n",
        "                    init.xavier_normal_(m.weight.data, gain=gain)\n",
        "                elif init_type == 'xavier_uniform':\n",
        "                    init.xavier_uniform_(m.weight.data, gain=1.0)\n",
        "                elif init_type == 'kaiming':\n",
        "                    init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "                elif init_type == 'orthogonal':\n",
        "                    init.orthogonal_(m.weight.data, gain=gain)\n",
        "                elif init_type == 'none':  # uses pytorch's default init method\n",
        "                    m.reset_parameters()\n",
        "                else:\n",
        "                    raise NotImplementedError(\"initialization method '{}' is not implemented\".format(init_type))\n",
        "                if hasattr(m, 'bias') and m.bias is not None:\n",
        "                    init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "        self.apply(init_func)\n",
        "\n",
        "    def forward(self, *inputs):\n",
        "        pass"
      ],
      "metadata": {
        "id": "AxsA-hcTd65p"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegGenerator(BaseNetwork):\n",
        "    def __init__(self, opt, input_nc, output_nc=13, norm_layer=nn.InstanceNorm2d):\n",
        "        super(SegGenerator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(input_nc, 64, kernel_size=3, padding=1), norm_layer(64), nn.ReLU(),\n",
        "                                   nn.Conv2d(64, 64, kernel_size=3, padding=1), norm_layer(64), nn.ReLU())\n",
        "\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=3, padding=1), norm_layer(128), nn.ReLU(),\n",
        "                                   nn.Conv2d(128, 128, kernel_size=3, padding=1), norm_layer(128), nn.ReLU())\n",
        "\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=3, padding=1), norm_layer(256), nn.ReLU(),\n",
        "                                   nn.Conv2d(256, 256, kernel_size=3, padding=1), norm_layer(256), nn.ReLU())\n",
        "\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(256, 512, kernel_size=3, padding=1), norm_layer(512), nn.ReLU(),\n",
        "                                   nn.Conv2d(512, 512, kernel_size=3, padding=1), norm_layer(512), nn.ReLU())\n",
        "\n",
        "        self.conv5 = nn.Sequential(nn.Conv2d(512, 1024, kernel_size=3, padding=1), norm_layer(1024), nn.ReLU(),\n",
        "                                   nn.Conv2d(1024, 1024, kernel_size=3, padding=1), norm_layer(1024), nn.ReLU())\n",
        "\n",
        "        self.up6 = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "                                 nn.Conv2d(1024, 512, kernel_size=3, padding=1), norm_layer(512), nn.ReLU())\n",
        "        self.conv6 = nn.Sequential(nn.Conv2d(1024, 512, kernel_size=3, padding=1), norm_layer(512), nn.ReLU(),\n",
        "                                   nn.Conv2d(512, 512, kernel_size=3, padding=1), norm_layer(512), nn.ReLU())\n",
        "\n",
        "        self.up7 = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "                                 nn.Conv2d(512, 256, kernel_size=3, padding=1), norm_layer(256), nn.ReLU())\n",
        "        self.conv7 = nn.Sequential(nn.Conv2d(512, 256, kernel_size=3, padding=1), norm_layer(256), nn.ReLU(),\n",
        "                                   nn.Conv2d(256, 256, kernel_size=3, padding=1), norm_layer(256), nn.ReLU())\n",
        "\n",
        "        self.up8 = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "                                 nn.Conv2d(256, 128, kernel_size=3, padding=1), norm_layer(128), nn.ReLU())\n",
        "        self.conv8 = nn.Sequential(nn.Conv2d(256, 128, kernel_size=3, padding=1), norm_layer(128), nn.ReLU(),\n",
        "                                   nn.Conv2d(128, 128, kernel_size=3, padding=1), norm_layer(128), nn.ReLU())\n",
        "\n",
        "        self.up9 = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "                                 nn.Conv2d(128, 64, kernel_size=3, padding=1), norm_layer(64), nn.ReLU())\n",
        "        self.conv9 = nn.Sequential(nn.Conv2d(128, 64, kernel_size=3, padding=1), norm_layer(64), nn.ReLU(),\n",
        "                                   nn.Conv2d(64, 64, kernel_size=3, padding=1), norm_layer(64), nn.ReLU(),\n",
        "                                   nn.Conv2d(64, output_nc, kernel_size=3, padding=1))\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.print_network()\n",
        "        self.init_weights(opt.init_type, opt.init_variance)\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv1(x)\n",
        "        conv2 = self.conv2(self.pool(conv1))\n",
        "        conv3 = self.conv3(self.pool(conv2))\n",
        "        conv4 = self.drop(self.conv4(self.pool(conv3)))\n",
        "        conv5 = self.drop(self.conv5(self.pool(conv4)))\n",
        "\n",
        "        conv6 = self.conv6(torch.cat((conv4, self.up6(conv5)), 1))\n",
        "        conv7 = self.conv7(torch.cat((conv3, self.up7(conv6)), 1))\n",
        "        conv8 = self.conv8(torch.cat((conv2, self.up8(conv7)), 1))\n",
        "        conv9 = self.conv9(torch.cat((conv1, self.up9(conv8)), 1))\n",
        "        return self.sigmoid(conv9)"
      ],
      "metadata": {
        "id": "Lov0pFnadeHw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seg = SegGenerator(opt, input_nc=opt.semantic_nc + 8, output_nc=opt.semantic_nc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yrev2QFjdTtJ",
        "outputId": "8e60ef72-4f99-4e46-e4c6-38424857c123"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network [SegGenerator] was created. Total number of parameters: 34.5 million. To see the architecture, do print(network).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def load_checkpoint(model, checkpoint_path):\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        raise ValueError(\"'{}' is not a valid checkpoint path\".format(checkpoint_path))\n",
        "    model.load_state_dict(torch.load(checkpoint_path))"
      ],
      "metadata": {
        "id": "VcQFfsLjn7Eg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_checkpoint(seg, os.path.join(opt.checkpoint_dir, opt.seg_checkpoint))\n",
        "# os.path.join(opt.checkpoint_dir, opt.seg_checkpoint)"
      ],
      "metadata": {
        "id": "hY_cc1pvnyYV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt.semantic_nc = 7\n",
        "parse_pred_down = seg(seg_input)\n",
        "print(parse_pred_down)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUCi_dm-dBEC",
        "outputId": "16dd5c78-07ef-4cd9-c1f1-795bea3fea2e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
            "           1.0000e+00, 9.9999e-01],\n",
            "          [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
            "           1.0000e+00, 1.0000e+00],\n",
            "          [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
            "           1.0000e+00, 1.0000e+00],\n",
            "          ...,\n",
            "          [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
            "           1.0000e+00, 1.0000e+00],\n",
            "          [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
            "           1.0000e+00, 1.0000e+00],\n",
            "          [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
            "           1.0000e+00, 1.0000e+00]],\n",
            "\n",
            "         [[8.0942e-07, 4.2455e-09, 8.4843e-09,  ..., 4.8573e-08,\n",
            "           4.2331e-09, 1.2142e-04],\n",
            "          [2.0676e-10, 7.7717e-13, 3.7571e-16,  ..., 1.7421e-14,\n",
            "           4.5607e-14, 2.4648e-08],\n",
            "          [1.9196e-10, 8.9014e-14, 4.5100e-18,  ..., 8.1787e-16,\n",
            "           9.1461e-15, 3.2984e-08],\n",
            "          ...,\n",
            "          [4.9883e-10, 1.3682e-13, 9.4887e-18,  ..., 4.0428e-19,\n",
            "           2.2653e-20, 7.4888e-11],\n",
            "          [6.5448e-11, 3.3202e-14, 3.5918e-19,  ..., 1.1840e-20,\n",
            "           5.5681e-21, 3.3974e-13],\n",
            "          [2.0571e-08, 1.9373e-11, 5.9169e-13,  ..., 1.0213e-14,\n",
            "           5.1807e-15, 3.8768e-09]],\n",
            "\n",
            "         [[1.0740e-11, 2.7202e-14, 9.6180e-16,  ..., 1.8702e-13,\n",
            "           1.5932e-13, 2.1675e-07],\n",
            "          [1.3072e-16, 1.2516e-19, 7.0971e-23,  ..., 7.6573e-20,\n",
            "           1.2355e-17, 2.9078e-10],\n",
            "          [2.8312e-18, 9.3910e-23, 1.8710e-26,  ..., 1.6489e-21,\n",
            "           1.2143e-17, 1.0451e-08],\n",
            "          ...,\n",
            "          [2.1935e-17, 1.5759e-21, 9.6915e-27,  ..., 2.4393e-26,\n",
            "           6.1499e-25, 9.9809e-15],\n",
            "          [2.3618e-19, 4.9039e-24, 6.1665e-27,  ..., 1.0176e-28,\n",
            "           9.6455e-26, 6.0316e-15],\n",
            "          [1.5740e-13, 1.2574e-18, 1.6110e-18,  ..., 2.2899e-20,\n",
            "           1.8032e-17, 1.3401e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[5.3576e-13, 1.6488e-18, 5.9020e-22,  ..., 3.0168e-18,\n",
            "           9.5589e-16, 5.8370e-11],\n",
            "          [1.4952e-18, 1.1770e-27, 1.5774e-35,  ..., 1.3695e-27,\n",
            "           1.3896e-22, 5.0496e-15],\n",
            "          [2.7465e-20, 1.8087e-32, 0.0000e+00,  ..., 5.6587e-32,\n",
            "           4.6569e-26, 3.5935e-16],\n",
            "          ...,\n",
            "          [2.0235e-21, 5.5159e-32, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.9078e-37, 6.2291e-24],\n",
            "          [9.0729e-20, 2.5651e-31, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           4.1914e-37, 2.6120e-24],\n",
            "          [1.7782e-13, 2.5759e-23, 7.5157e-30,  ..., 3.5007e-29,\n",
            "           4.0048e-28, 3.2859e-18]],\n",
            "\n",
            "         [[3.0410e-10, 1.1405e-16, 2.3836e-18,  ..., 1.1864e-14,\n",
            "           1.0770e-13, 6.5540e-09],\n",
            "          [9.1452e-14, 3.8605e-24, 4.0906e-29,  ..., 1.8520e-21,\n",
            "           9.6476e-19, 7.2655e-12],\n",
            "          [8.2897e-15, 1.8497e-27, 2.4051e-35,  ..., 1.9327e-24,\n",
            "           7.8738e-21, 1.4323e-12],\n",
            "          ...,\n",
            "          [3.1577e-16, 2.7411e-28, 6.8303e-35,  ..., 8.6392e-33,\n",
            "           5.7611e-33, 8.1773e-20],\n",
            "          [9.3247e-15, 2.6834e-26, 3.1822e-33,  ..., 8.9956e-32,\n",
            "           1.4112e-31, 5.1740e-20],\n",
            "          [3.1764e-10, 6.4276e-19, 3.5278e-24,  ..., 2.7365e-23,\n",
            "           6.9512e-23, 5.9379e-15]],\n",
            "\n",
            "         [[2.2714e-17, 5.7310e-23, 6.8226e-26,  ..., 1.8022e-22,\n",
            "           1.9401e-16, 1.6306e-10],\n",
            "          [8.2027e-26, 1.7191e-36, 0.0000e+00,  ..., 7.8358e-35,\n",
            "           5.7909e-25, 1.2686e-15],\n",
            "          [2.7739e-28, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           2.9142e-29, 1.3395e-17],\n",
            "          ...,\n",
            "          [1.2322e-30, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 9.2621e-26],\n",
            "          [2.6114e-27, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 2.4574e-26],\n",
            "          [1.3186e-18, 3.5687e-30, 6.2139e-38,  ..., 2.6721e-38,\n",
            "           5.9437e-33, 3.1638e-21]]]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gauss = tgm.image.GaussianBlur((15, 15), (3, 3))\n",
        "up = nn.Upsample(size=(opt.load_height, opt.load_width), mode='bilinear')\n",
        "parse_pred = gauss(up(parse_pred_down))"
      ],
      "metadata": {
        "id": "pJ6QhwNXf924"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parse_pred = parse_pred.argmax(dim=1)[:, None]\n",
        "\n",
        "parse_old = torch.zeros(parse_pred.size(0), 13, opt.load_height, opt.load_width, dtype=torch.float)\n",
        "parse_old.scatter_(1, parse_pred, 1.0)\n",
        "\n",
        "labels = {\n",
        "    0:  ['background',  [0]],\n",
        "    1:  ['paste',       [2, 4, 7, 8, 9, 10, 11]],\n",
        "    2:  ['upper',       [3]],\n",
        "    3:  ['hair',        [1]],\n",
        "    4:  ['left_arm',    [5]],\n",
        "    5:  ['right_arm',   [6]],\n",
        "    6:  ['noise',       [12]]\n",
        "}\n",
        "parse = torch.zeros(parse_pred.size(0), 7, opt.load_height, opt.load_width, dtype=torch.float)\n",
        "for j in range(len(labels)):\n",
        "    for label in labels[j][1]:\n",
        "        parse[:, j] += parse_old[:, label]"
      ],
      "metadata": {
        "id": "tT2BSWehfygo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parse_pred_down.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIF7X-TbfZdM",
        "outputId": "966f29b8-cd13-4d82-fc46-e0000bfe97fc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 13, 256, 192])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parse_pred_down[0][5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09EkSjCimGlR",
        "outputId": "00537bc6-24a7-48f6-f726-ace80b953bfc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.4121e-10, 1.0682e-14, 2.6177e-15,  ..., 4.9330e-12, 1.3673e-11,\n",
              "         3.7145e-06],\n",
              "        [7.7245e-17, 6.2063e-25, 1.3710e-28,  ..., 1.6009e-21, 2.3344e-18,\n",
              "         8.3675e-10],\n",
              "        [7.3827e-18, 3.3736e-28, 2.4384e-33,  ..., 3.6336e-23, 7.0877e-19,\n",
              "         6.7631e-09],\n",
              "        ...,\n",
              "        [2.9037e-18, 1.8181e-26, 6.8742e-31,  ..., 4.7400e-30, 1.0927e-28,\n",
              "         1.4477e-15],\n",
              "        [3.7710e-19, 5.7252e-29, 2.8240e-33,  ..., 1.4691e-33, 2.0947e-31,\n",
              "         1.7440e-17],\n",
              "        [3.3315e-12, 8.1505e-20, 5.2073e-22,  ..., 4.2068e-23, 2.3035e-21,\n",
              "         7.6269e-12]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parse_pred_down = parse_pred_down.detach().numpy()"
      ],
      "metadata": {
        "id": "KxZrd2K8mYYH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parse_pred_down[0][5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfjK576rmiq_",
        "outputId": "b212802a-41f1-43d1-f34e-3a1bdec481a3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.4121380e-10, 1.0682497e-14, 2.6176886e-15, ..., 4.9330358e-12,\n",
              "        1.3673257e-11, 3.7145046e-06],\n",
              "       [7.7244792e-17, 6.2063188e-25, 1.3709647e-28, ..., 1.6009193e-21,\n",
              "        2.3343620e-18, 8.3674906e-10],\n",
              "       [7.3826771e-18, 3.3736038e-28, 2.4384058e-33, ..., 3.6335650e-23,\n",
              "        7.0876707e-19, 6.7631367e-09],\n",
              "       ...,\n",
              "       [2.9037311e-18, 1.8181273e-26, 6.8742040e-31, ..., 4.7400397e-30,\n",
              "        1.0926699e-28, 1.4477220e-15],\n",
              "       [3.7710172e-19, 5.7252432e-29, 2.8239643e-33, ..., 1.4690603e-33,\n",
              "        2.0946752e-31, 1.7440475e-17],\n",
              "       [3.3314601e-12, 8.1505137e-20, 5.2072867e-22, ..., 4.2068008e-23,\n",
              "        2.3034952e-21, 7.6268505e-12]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Image.fromarray(np.uint8(parse_pred_down[0][12] * 255))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "Z30AK_slitxN",
        "outputId": "a569e46a-cd5b-48da-8512-0aad658ff372"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=192x256>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMAAAAEACAAAAADAKCbbAAAAR0lEQVR4nO3BMQEAAADCoPVPbQwfoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgLcBwQAAAZ7EQF4AAAAASUVORK5CYII=\n"
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parse.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CByRfNqqaMz",
        "outputId": "48267312-c5eb-45a1-f649-94f52b86a3f8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 1024, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0:  ['background',  [0]],\n",
        "# 1:  ['paste',       [2, 4, 7, 8, 9, 10, 11]],\n",
        "# 2:  ['upper',       [3]],\n",
        "# 3:  ['hair',        [1]],\n",
        "# 4:  ['left_arm',    [5]],\n",
        "# 5:  ['right_arm',   [6]],\n",
        "# 6:  ['noise',       [12]]\n",
        "seg_op = np.uint8(parse[0][2])\n",
        "for i in range(3, 6):\n",
        "    seg_op = seg_op + np.uint8(parse[0][i])\n",
        "Image.fromarray(seg_op * 255)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-JCKxiiMqan4",
        "outputId": "eb4e4550-a6f0-41ce-85d0-d05a447f4d2b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=768x1024>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_image = Image.fromarray(np.uint8(parse[0][5]) * 255)\n",
        "rgb_image = tmp_image.convert(\"RGB\")\n",
        "red_color = (0, 150, 255)\n",
        "for x in range(tmp_image.width):\n",
        "    for y in range(tmp_image.height):\n",
        "        pixel_value = tmp_image.getpixel((x, y))\n",
        "        if pixel_value == 255:  # Assuming 255 represents the thresholded pixels (white)\n",
        "            rgb_image.putpixel((x, y), red_color)\n",
        "        else:\n",
        "            rgb_image.putpixel((x, y), (0, 0, 0))  # Set non-thresholded pixels to black\n",
        "rgb_image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xIxRE9ZUuzKM",
        "outputId": "17e510ed-9ba5-4289-a596-ccb750538882"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=768x1024>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jXmhMPdcwdW0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}